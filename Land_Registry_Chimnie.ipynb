{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "collapsed_sections": [
        "_SwmZ4z7ZWQk",
        "5KGo68v3Y_dm",
        "JoX625DSmc1u",
        "cnBv1i713UO6",
        "wtc-4_8WZDci"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "!python -m pip install statsmodels\n",
        "!pip install folium"
      ],
      "metadata": {
        "id": "73Y7xMoB1kce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNOY-gFkqx_T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_headers = [\n",
        "    \"Transaction_unique_identifier\",\n",
        "    \"Price\",\n",
        "    \"Date_of_Transfer\",\n",
        "    \"Postcode\",\n",
        "    \"Property_Type\",\n",
        "    \"Old/New\",\n",
        "    \"Duration\",\n",
        "    \"PAON\",\n",
        "    \"SAON\",\n",
        "    \"Street\",\n",
        "    \"Locality\",\n",
        "    \"Town/City\",\n",
        "    \"District\",\n",
        "    \"County\",\n",
        "    \"PPD_Category_Type\",\n",
        "    \"Record_Status_monthly_file_only\"\n",
        "]"
      ],
      "metadata": {
        "id": "n4enUhWgwPSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_main = pd.read_csv('/content/drive/MyDrive/Chimnie/pp-complete_2.csv',names = column_headers)"
      ],
      "metadata": {
        "id": "-sP3wpgMrHh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_main.copy()"
      ],
      "metadata": {
        "id": "4KL-fu1trRc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert columns to appropriate datatypes\n",
        "df['Transaction_unique_identifier'] = df['Transaction_unique_identifier'].astype('string')\n",
        "df['Price'] = df['Price'].astype('float')\n",
        "df['Date_of_Transfer'] = pd.to_datetime(df['Date_of_Transfer'])\n",
        "df['Postcode'] = df['Postcode'].astype('string')\n",
        "df['Property_Type'] = df['Property_Type'].astype('category')\n",
        "df['Old/New'] = df['Old/New'].astype('category')\n",
        "df['Duration'] = df['Duration'].astype('category')\n",
        "df['PAON'] = df['PAON'].astype('string')\n",
        "df['SAON'] = df['SAON'].astype('string')\n",
        "df['Street'] = df['Street'].astype('string')\n",
        "df['Locality'] = df['Locality'].astype('string')\n",
        "df['Town/City'] = df['Town/City'].astype('string')\n",
        "df['District'] = df['District'].astype('string')\n",
        "df['County'] = df['County'].astype('string')\n",
        "df['PPD_Category_Type'] = df['PPD_Category_Type'].astype('category')\n",
        "df['Record_Status_monthly_file_only'] = df['Record_Status_monthly_file_only'].astype('category')"
      ],
      "metadata": {
        "id": "rxpaF9L74Uwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "QK5CIqW9MWXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "SHDnYA8w-4Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nspl_df_main = pd.read_csv('/content/drive/MyDrive/Chimnie/NSPL_2021_MAY_2024/Data/NSPL21_MAY_2024_UK.csv')"
      ],
      "metadata": {
        "id": "cf5QQViLt78L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nspl_df = nspl_df_main.copy()"
      ],
      "metadata": {
        "id": "vb3GJ4oozEtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nspl_df['pcd'] = nspl_df['pcd'].astype('string')\n",
        "nspl_df['pcd2'] = nspl_df['pcd2'].astype('string')\n",
        "nspl_df['pcds'] = nspl_df['pcds'].astype('string')\n",
        "nspl_df['laua'] = nspl_df['laua'].astype('string')\n",
        "nspl_df['rgn'] = nspl_df['rgn'].astype('string')"
      ],
      "metadata": {
        "id": "u0mSo59CGQ_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nspl_df.isna().sum()"
      ],
      "metadata": {
        "id": "xmD53bQ0K3FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nspl_df.info()"
      ],
      "metadata": {
        "id": "Hg4SPfFiuLB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all spaces in the 'pcd', 'pcd2', and 'pcds' columns\n",
        "nspl_df['pcd'] = nspl_df['pcd'].str.replace(r'\\s+', '', regex=True)\n",
        "nspl_df['pcd2'] = nspl_df['pcd2'].str.replace(r'\\s+', '', regex=True)\n",
        "nspl_df['pcds'] = nspl_df['pcds'].str.replace(r'\\s+', '', regex=True)"
      ],
      "metadata": {
        "id": "VZPn9XtOH7qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nspl_df[(nspl_df['pcd'] == nspl_df['pcd2']) & (nspl_df['pcd2'] == nspl_df['pcds'])])"
      ],
      "metadata": {
        "id": "zal79oLHGFPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n"
      ],
      "metadata": {
        "id": "_SwmZ4z7ZWQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "london_boroughs = nspl_df[nspl_df['laua'].str.startswith('E09')]"
      ],
      "metadata": {
        "id": "3I4cXouEzBOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(london_boroughs)"
      ],
      "metadata": {
        "id": "i28zJ6Kp-xhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Postcode'] = df['Postcode'].str.replace(r'\\s+', '', regex=True)"
      ],
      "metadata": {
        "id": "Vy7jd_fWNFq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023 = df[df['Date_of_Transfer'].dt.year == 2023]\n",
        "\n",
        "# Merge with NSPL dataset to get borough information\n",
        "filtered_df_2023 = df_2023[df_2023['Postcode'].isin(london_boroughs['pcd'])]\n"
      ],
      "metadata": {
        "id": "P5XWupdg9vlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "borough_sales = filtered_df_2023.groupby('District').agg(\n",
        "    Sales_Count=('Price', 'size'),\n",
        "    Average_Price=('Price', 'mean')\n",
        ").reset_index()"
      ],
      "metadata": {
        "id": "hYhr1i82JZKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "borough_sales['District'] = borough_sales['District'].replace('CITY OF WESTMINSTER', 'WESTMINSTER')\n",
        "\n",
        "borough_sales"
      ],
      "metadata": {
        "id": "JSQm5IJa_SjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf = gpd.read_file('/content/drive/MyDrive/Chimnie/london_boroughs.geojson')"
      ],
      "metadata": {
        "id": "crdNMMVbCC_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf['name'] = gdf['name'].str.upper()\n",
        "gdf"
      ],
      "metadata": {
        "id": "BSzsqdTFD-MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_gdf = gdf.set_index('name').join(borough_sales.set_index('District'))\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(1, 1, figsize=(20, 15))\n",
        "merged_gdf.plot(column='Average_Price', cmap='Accent', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "\n",
        "# Add titles and labels\n",
        "ax.set_title('Average Price in London Borough - 2023', fontdict={'fontsize': '10', 'fontweight': '5'})\n",
        "ax.set_xlabel('Longitude', fontsize=12)\n",
        "ax.set_ylabel('Latitude', fontsize=12)\n",
        "\n",
        "\n",
        "offsets = {\n",
        "    'KENSINGTON AND CHELSEA': (0, -3),\n",
        "    'HAMMERSMITH AND FULHAM': (0, -12)\n",
        "}\n",
        "for idx, row in merged_gdf.iterrows():\n",
        "    offset = offsets.get(row.name, (0, 0))\n",
        "    plt.annotate(\n",
        "        text=f\"{row.name}\\n{row.Average_Price:.2f}\",\n",
        "        xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n",
        "        xytext=offset,\n",
        "        textcoords='offset points',\n",
        "        ha='center',\n",
        "        va='center',\n",
        "        fontsize=7,\n",
        "        color='black',\n",
        "        bbox=dict(facecolor='white', alpha=0.5, edgecolor='none', pad=1)  # Optional for better readability\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eujV024uC95n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(20, 15))\n",
        "merged_gdf.plot(column='Sales_Count', cmap='spring', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "\n",
        "ax.set_title('Sales Count in London Borough - 2023', fontdict={'fontsize': '10', 'fontweight': '5'})\n",
        "ax.set_xlabel('Longitude', fontsize=12)\n",
        "ax.set_ylabel('Latitude', fontsize=12)\n",
        "\n",
        "\n",
        "offsets = {\n",
        "    'HAMMERSMITH AND FULHAM': (0, -12)\n",
        "}\n",
        "for idx, row in merged_gdf.iterrows():\n",
        "    offset = offsets.get(row.name, (0, 0))\n",
        "    plt.annotate(\n",
        "        text=f\"{row.name}\\n{row.Sales_Count}\",\n",
        "        xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n",
        "        xytext=offset,\n",
        "        textcoords='offset points',\n",
        "        ha='center',\n",
        "        va='center',\n",
        "        fontsize=7,\n",
        "        color='black',\n",
        "        bbox=dict(facecolor='white', alpha=0.5, edgecolor='none', pad=1)  # Optional for better readability\n",
        "    )\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "olQKqvl_LuV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n"
      ],
      "metadata": {
        "id": "5KGo68v3Y_dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter year from 2020\n",
        "df_from_2020 = df[df['Date_of_Transfer'].dt.year >= 2020]\n",
        "\n",
        "#filter Property_Type as 'F' (Flat) and it should be new\n",
        "df_from_2020 = df_from_2020[(df_from_2020['Property_Type'] == 'F') & (df_from_2020['Old/New'] == 'Y')]\n"
      ],
      "metadata": {
        "id": "fE3a-L6PSxvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_from_2020.describe()"
      ],
      "metadata": {
        "id": "vI6OlIjzSxrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_df = nspl_df.copy()\n",
        "region_df = region_df.dropna(subset=['rgn'])\n",
        "\n",
        "def determine_region(rgn):\n",
        "    if rgn.startswith('E12'):\n",
        "        return 'England'\n",
        "    elif rgn.startswith('W'):\n",
        "        return 'Wales'\n",
        "    elif rgn.startswith('S'):\n",
        "        return 'Scotland'\n",
        "    elif rgn.startswith('N'):\n",
        "        return 'Northern Ireland'\n",
        "    elif rgn.startswith('L'):\n",
        "        return 'Channel Islands'\n",
        "    elif rgn.startswith('M'):\n",
        "        return 'Isle of Man'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# Apply the function to the 'rgn' column to create the 'Region' column\n",
        "region_df['Region'] = region_df['rgn'].apply(determine_region)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5ItkEw1XSxpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_df_from_2020 = df_from_2020.merge(region_df, left_on='Postcode', right_on='pcd', how='left')"
      ],
      "metadata": {
        "id": "bJovhIOnSxm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_df_from_2020['Region'].value_counts()"
      ],
      "metadata": {
        "id": "0I9ReeDqkedB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Postcode lookup has 6 Regions:\n",
        "England\n",
        "\n",
        "1.   England\n",
        "2.   Wales\n",
        "\n",
        "1.   Scotland\n",
        "2.   Northern Ireland\n",
        "\n",
        "1.   Channel Islands\n",
        "2.   Isle of Man\n",
        "\n",
        "But the Price Paid Data contains only the property sales in **England** and **Wales**"
      ],
      "metadata": {
        "id": "2z_4nQNYk8rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_sales = region_df_from_2020.groupby('Region').agg(\n",
        "    New_Build_Flats_Sold=('Transaction_unique_identifier', 'count')\n",
        ").reset_index()\n",
        "\n",
        "print(region_sales)"
      ],
      "metadata": {
        "id": "ZHB-i_JpkZFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_df_from_2020['Region'].value_counts()"
      ],
      "metadata": {
        "id": "5eWAXbcHSxeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4"
      ],
      "metadata": {
        "id": "JoX625DSmc1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2020_onwards = df[df['Date_of_Transfer'] >= '2020-01-01']\n",
        "\n",
        "df_2020_onwards['Week'] = df_2020_onwards['Date_of_Transfer'].dt.to_period('W').astype(str)\n"
      ],
      "metadata": {
        "id": "sTuk6G6hopjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of sales per week\n",
        "weekly_sales = df_2020_onwards.groupby('Week').size().reset_index(name='Sales_Count')"
      ],
      "metadata": {
        "id": "yGGL_r4Yopfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_sales['Week'].nunique()"
      ],
      "metadata": {
        "id": "qPQb881_1Nnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a line chart of sales per week\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(weekly_sales['Week'], weekly_sales['Sales_Count'], marker='o')\n",
        "plt.title('Number of Sales Per Week Since the Start of 2020')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Number of Sales')\n",
        "\n",
        "# Set x-ticks at intervals\n",
        "num_weeks = len(weekly_sales['Week'])\n",
        "plt.xticks(weekly_sales['Week'][::5], rotation=90)  # Adjust interval as needed\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "37eYLGkhopdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_price_per_week = df_2020_onwards.groupby('Week')['Price'].mean().reset_index(name='Average_Price')"
      ],
      "metadata": {
        "id": "RdecnyKC3adg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a line chart of sales per week\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(avg_price_per_week['Week'], avg_price_per_week['Average_Price'], marker='o')\n",
        "plt.title('Avg price Per Week Since the Start of 2020')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Avg Price')\n",
        "\n",
        "# Set x-ticks at intervals\n",
        "num_weeks = len(weekly_sales['Week'])\n",
        "plt.xticks(weekly_sales['Week'][::5], rotation=90)  # Adjust interval as needed\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0_NuRMvXopam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "# Plot the average price per week on the left y-axis\n",
        "ax1.plot(avg_price_per_week['Week'], avg_price_per_week['Average_Price'], marker='o', color='blue', label='Average Price')\n",
        "ax1.set_xlabel('Week')\n",
        "ax1.set_ylabel('Average Price', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# Set x-ticks at intervals\n",
        "ax1.set_xticks(avg_price_per_week['Week'][::5])\n",
        "ax1.set_xticklabels(avg_price_per_week['Week'][::5], rotation=90)  # Adjust interval as needed\n",
        "\n",
        "# Create a second y-axis sharing the same x-axis\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(weekly_sales['Week'], weekly_sales['Sales_Count'], marker='o', color='green', label='Number of Sales')\n",
        "ax2.set_ylabel('Number of Sales', color='green')\n",
        "ax2.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "# Set title and layout\n",
        "plt.title('Average Price and Number of Sales Per Week Since the Start of 2020')\n",
        "fig.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E8hOUO8PopYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the moving average\n",
        "weekly_sales['Sales_MA'] = weekly_sales['Sales_Count'].rolling(window=4).mean()\n",
        "\n",
        "# Plot the data with moving average\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(weekly_sales['Week'], weekly_sales['Sales_Count'], marker='o', label='Weekly Sales')\n",
        "plt.plot(weekly_sales['Week'], weekly_sales['Sales_MA'], color='red', label='4-Week Moving Average')\n",
        "plt.title('Number of Sales Per Week Since the Start of 2020 with Moving Average')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Number of Sales')\n",
        "plt.xticks(weekly_sales['Week'][::5], rotation=90)  # Adjust interval as needed\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TKGdprU3opVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Convert 'Week' to datetime format for decomposition\n",
        "weekly_sales['Week_Start'] = pd.to_datetime(weekly_sales['Week'].apply(lambda x: x.split('/')[0]))\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(weekly_sales.set_index('Week_Start')['Sales_Count'], model='additive', period=52)\n",
        "\n",
        "# Plot the decomposition\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 12))\n",
        "decomposition.observed.plot(ax=ax1, title='Observed')\n",
        "decomposition.trend.plot(ax=ax2, title='Trend')\n",
        "decomposition.seasonal.plot(ax=ax3, title='Seasonal')\n",
        "decomposition.resid.plot(ax=ax4, title='Residual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TsYNwHG0opS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backup_weekly_sales= weekly_sales.copy()"
      ],
      "metadata": {
        "id": "2Bem_j5FMGVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5\n"
      ],
      "metadata": {
        "id": "cnBv1i713UO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['Price'], bins=20, edgecolor='black')\n",
        "plt.title('Distribution of Sale Prices')\n",
        "plt.xlabel('Sale Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nvbZ4GOc3bFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate statistics\n",
        "mean_price = np.mean(df['Price'])\n",
        "median_price = np.median(df['Price'])\n",
        "std_dev_price = np.std(df['Price'])\n",
        "range_price = np.ptp(df['Price'])\n",
        "skewness = df['Price'].skew()\n",
        "kurtosis = df['Price'].kurt()\n",
        "\n",
        "# Print the statistics\n",
        "print(f'Mean Sale Price: {mean_price}')\n",
        "print(f'Median Sale Price: {median_price}')\n",
        "print(f'Standard Deviation of Sale Prices: {std_dev_price}')\n",
        "print(f'Range of Sale Prices: {range_price}')\n",
        "print(f'Skewness of Sale Prices: {skewness}')\n",
        "print(f'Kurtosis of Sale Prices: {kurtosis}')\n"
      ],
      "metadata": {
        "id": "vGgrzBF-3bBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.boxplot(df['Price'])\n",
        "plt.title('Box Plot of Price')\n",
        "plt.xlabel('Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UcUrqdisAkz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_99_percentile = df['Price'].quantile(0.99)\n",
        "\n",
        "# Filter the DataFrame to include only data up to the 99th percentile\n",
        "filtered_df = df[df['Price'] <= price_99_percentile]\n",
        "\n",
        "# Plot the box plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.boxplot(filtered_df['Price'])\n",
        "plt.title('Box Plot of Price up to the 99th Percentile')\n",
        "plt.xlabel('Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YMFfHIKfBzR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation to Sale_Price\n",
        "df['Log_Sale_Price'] = np.log(df['Price'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['Log_Sale_Price'], kde=True, bins=30)\n",
        "plt.title('Distribution of Log-Transformed Sale Prices')\n",
        "plt.xlabel('Log Sale Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HUZQITUxHSQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a threshold to identify outliers (e.g., values above the 99th percentile)\n",
        "threshold = df['Price'].quantile(0.99)\n",
        "df_filtered = df[df['Price'] < threshold]\n",
        "\n",
        "# Plot the histogram of the filtered sale prices\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_filtered['Price'], bins=20, edgecolor='black')\n",
        "plt.title('Distribution of Sale Prices (Filtered)')\n",
        "plt.xlabel('Sale Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MQAfW_FI3a8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Log-Transformed Data Analysis\n",
        "mean_log_price = np.mean(df['Log_Sale_Price'])\n",
        "median_log_price = np.median(df['Log_Sale_Price'])\n",
        "std_dev_log_price = np.std(df['Log_Sale_Price'])\n",
        "skewness_log = df['Log_Sale_Price'].skew()\n",
        "kurtosis_log = df['Log_Sale_Price'].kurt()\n",
        "\n",
        "print(f'Mean Log Sale Price: {mean_log_price}')\n",
        "print(f'Median Log Sale Price: {median_log_price}')\n",
        "print(f'Standard Deviation of Log Sale Prices: {std_dev_log_price}')\n",
        "print(f'Skewness of Log Sale Prices: {skewness_log}')\n",
        "print(f'Kurtosis of Log Sale Prices: {kurtosis_log}')\n",
        "\n",
        "# Filtered Data Analysis\n",
        "mean_filtered_price = np.mean(df_filtered['Price'])\n",
        "median_filtered_price = np.median(df_filtered['Price'])\n",
        "std_dev_filtered_price = np.std(df_filtered['Price'])\n",
        "skewness_filtered = df_filtered['Price'].skew()\n",
        "kurtosis_filtered = df_filtered['Price'].kurt()\n",
        "\n",
        "print(f'Mean Filtered Sale Price: {mean_filtered_price}')\n",
        "print(f'Median Filtered Sale Price: {median_filtered_price}')\n",
        "print(f'Standard Deviation of Filtered Sale Prices: {std_dev_filtered_price}')\n",
        "print(f'Skewness of Filtered Sale Prices: {skewness_filtered}')\n",
        "print(f'Kurtosis of Filtered Sale Prices: {kurtosis_filtered}')\n"
      ],
      "metadata": {
        "id": "LJ0RKUM53a6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6 & 7\n"
      ],
      "metadata": {
        "id": "wtc-4_8WZDci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf1 = gpd.read_file('/content/drive/MyDrive/Chimnie/OSGB_Grids-master/GeoJSON/OSGB_Grid_10km.geojson')"
      ],
      "metadata": {
        "id": "WYdLaSz2b4Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf1.info()"
      ],
      "metadata": {
        "id": "egJkeQjnb4Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gdf1.crs)"
      ],
      "metadata": {
        "id": "wjErgkWdb4Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf1.head()"
      ],
      "metadata": {
        "id": "171HXuvmkt6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_geometry = gdf1.loc[0, 'geometry']\n",
        "print(f\"Geometry Type: {first_geometry.geom_type}\")\n",
        "print(f\"Geometry Coordinates: {first_geometry}\")\n",
        "\n",
        "# Loop through the first few geometries and print their types and coordinates\n",
        "print(gdf1.loc[0, 'geometry'].__geo_interface__)\n"
      ],
      "metadata": {
        "id": "ii-7ar5chU86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Postcode'] = df['Postcode'].str.replace(r'\\s+', '', regex=True)"
      ],
      "metadata": {
        "id": "HdSTgnhSibjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df.merge(nspl_df[['pcd', 'lat', 'long', 'oseast1m','osnrth1m']],left_on='Postcode', right_on='pcd', how='left')"
      ],
      "metadata": {
        "id": "ajwOPbFOijLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head()"
      ],
      "metadata": {
        "id": "lQjVWvySkINN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_new[['lat','long']].isna().sum())\n",
        "\n",
        "df_new = df_new.dropna(subset=['lat'])\n",
        "df_new.isna().sum()"
      ],
      "metadata": {
        "id": "7RgKIIIVmTqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import Point\n",
        "geometry = [Point(xy) for xy in zip(df_new['long'], df_new['lat'])]\n",
        "new_gdf = gpd.GeoDataFrame(df_new, geometry=geometry)"
      ],
      "metadata": {
        "id": "5x50W4W3lVhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_gdf.set_crs(epsg=4326, inplace=True)\n",
        "gdf1.set_crs(epsg=4326, inplace=True)"
      ],
      "metadata": {
        "id": "gvPw540rlVeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_gdf = gpd.sjoin(new_gdf, gdf1[['TILE_NAME', 'geometry']], how='left', predicate='within')"
      ],
      "metadata": {
        "id": "cpiCpCKSsboO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_gdf.head()"
      ],
      "metadata": {
        "id": "-vj3iVdCsbit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_gdf['TILE_NAME'].value_counts()"
      ],
      "metadata": {
        "id": "ZCzRw3WFt1q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_gdf = new_gdf.groupby('TILE_NAME').agg(\n",
        "    total_sales=('Transaction_unique_identifier', 'count'),\n",
        "    Average_Price=('Price', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "print(sales_gdf)"
      ],
      "metadata": {
        "id": "auqhIg-_sbkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_gdf = gdf1.merge(sales_gdf, left_on = 'TILE_NAME', right_on='TILE_NAME', how='left')"
      ],
      "metadata": {
        "id": "968GTHRlviS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merged_gdf = merged_gdf.to_crs('EPSG:27700')\n",
        "merged_gdf.crs"
      ],
      "metadata": {
        "id": "ErSapuB7Ly-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_gdf.head()"
      ],
      "metadata": {
        "id": "MHGF7qv1wsGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(30, 30))\n",
        "merged_gdf.plot(column='total_sales', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "#merged_gdf.plot(column='REGION', ax=ax, legend=True, cmap='Set3', edgecolor='k', linewidth=0.8)\n",
        "ax.set_title('total_sales', fontdict={'fontsize': '20', 'fontweight': '5'})\n",
        "ax.set_xlabel('Longitude', fontsize=15)\n",
        "ax.set_ylabel('Latitude', fontsize=15)\n",
        "\n",
        "\n",
        "for idx, row in merged_gdf.iterrows():\n",
        "    offset = offsets.get(row.name, (0, 0))\n",
        "    plt.annotate(\n",
        "        text=f\"{row.total_sales}\",\n",
        "        xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n",
        "        xytext=offset,\n",
        "        textcoords='offset points',\n",
        "        ha='center',\n",
        "        va='center',\n",
        "        fontsize=5,\n",
        "        color='black',\n",
        "        bbox=dict(facecolor='white', alpha=0.5, edgecolor='none', pad=1)\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "I3-8UpRcILOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(30, 30))\n",
        "merged_gdf.plot(column='Average_Price', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "#merged_gdf.plot(column='REGION', ax=ax, legend=True, cmap='Set3', edgecolor='k', linewidth=0.8)\n",
        "ax.set_title('Average Price', fontdict={'fontsize': '20', 'fontweight': '5'})\n",
        "ax.set_xlabel('Longitude', fontsize=15)\n",
        "ax.set_ylabel('Latitude', fontsize=15)\n",
        "\n",
        "\n",
        "for idx, row in merged_gdf.iterrows():\n",
        "    offset = offsets.get(row.name, (0, 0))\n",
        "    plt.annotate(\n",
        "        text=f\"{row.Average_Price}\",\n",
        "        xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n",
        "        xytext=offset,\n",
        "        textcoords='offset points',\n",
        "        ha='center',\n",
        "        va='center',\n",
        "        fontsize=5,\n",
        "        color='black',\n",
        "        bbox=dict(facecolor='white', alpha=0.5, edgecolor='none', pad=1)\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "OzNcMtMc0L8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_region(row):\n",
        "    if row['ENGLAND'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'England'\n",
        "    elif row['SCOTLAND'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'Scotland'\n",
        "    elif row['WALES'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'Wales'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "merged_gdf['REGION'] = merged_gdf.apply(determine_region, axis=1)\n",
        "\n",
        "\n",
        "bounds = [[49.5, -10.5], [61, 2]]  # Approximate bounds for the UK\n",
        "\n",
        "\n",
        "\n",
        "# Create a folium map centered around the bounds of the UK\n",
        "m = folium.Map(location=[55, -3], zoom_start=6, max_bounds=True, tiles='CartoDB positron')\n",
        "m.fit_bounds(bounds)\n",
        "\n",
        "# Add polygons to the map\n",
        "folium.GeoJson(\n",
        "    merged_gdf.to_json(),\n",
        "    name='Polygons',\n",
        "    style_function=lambda x: {\n",
        "\n",
        "        'weight': 1,\n",
        "        'fillOpacity': 0.1,\n",
        "    },\n",
        "    tooltip=folium.GeoJsonTooltip(fields=['TILE_NAME', 'total_sales','Average_Price'])\n",
        ").add_to(m)\n",
        "\n",
        "# Prepare data for heat map\n",
        "heat_data = [[point.xy[1][0], point.xy[0][0], row['total_sales']]\n",
        "             for index, row in merged_gdf.iterrows()\n",
        "             for point in [row['geometry'].centroid]\n",
        "             if not pd.isnull(row['total_sales'])]\n",
        "\n",
        "# Add heat map\n",
        "HeatMap(heat_data).add_to(m)\n",
        "\n",
        "# Add layer control\n",
        "folium.LayerControl().add_to(m)\n",
        "\n",
        "# Save the map to an HTML file and display it\n",
        "m.save('map.html')\n",
        "m"
      ],
      "metadata": {
        "id": "tLfCiABwyIHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 9"
      ],
      "metadata": {
        "id": "5EG69DEoLcI2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceHyuACRyH-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HBtRR_0ayHrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_region(row):\n",
        "    if row['ENGLAND'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'England'\n",
        "    elif row['SCOTLAND'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'Scotland'\n",
        "    elif row['WALES'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'Wales'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "gdf1['REGION'] = gdf1.apply(determine_region, axis=1)\n",
        "\n",
        "# Plotting the GeoDataFrame with different colors for each region\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "gdf1.plot(column='REGION', ax=ax, legend=True, cmap='Set3', edgecolor='k')\n",
        "\n",
        "# Setting title and labels\n",
        "ax.set_title('10km Map of England, Scotland, and Wales')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sqMpq3u5u6-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yOXUvHT_u66v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mz1Pszfpu64e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5cwwfiVu62F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1SzG2VrsbgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the first five geometries\n",
        "gdf1.head(2).plot()\n",
        "plt.title('First Five Geometries')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gFwZrCS6gPXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf1.loc[1,'geometry']\n",
        "\n",
        "gdf1.loc[[1]].plot()\n",
        "plt.title('Polygon for Record 0')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eA8ozncY3a1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnCGxSOwZJEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yXVMukGZJBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mvh7gJ0yZI-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jPE22hBqZI8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_sales= backup_weekly_sales.copy()"
      ],
      "metadata": {
        "id": "cGjgI3pqSF7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "weekly_sales= backup_weekly_sales.copy()\n",
        "\n",
        "weekly_sales = weekly_sales.drop('Sales_MA', axis=1)\n",
        "# Set the index to Week_Start for time series analysis\n",
        "weekly_sales.set_index('Week_Start', inplace=True)\n",
        "\n",
        "# Perform the Augmented Dickey-Fuller test\n",
        "result = adfuller(weekly_sales['Sales_Count'].dropna())\n",
        "print('ADF Statistic:', result[0])\n",
        "print('p-value:', result[1])\n"
      ],
      "metadata": {
        "id": "d_miKocjil9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot with EPSG:4326\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
        "\n",
        "gdf_4326 = gdf1.to_crs(epsg=4326)\n",
        "gdf_4326.plot(ax=ax[0], edgecolor='k')\n",
        "ax[0].set_title('WGS 84 (EPSG:4326)')\n",
        "\n",
        "# Plot with EPSG:27700\n",
        "gdf_27700 = gdf1.to_crs(epsg=27700)\n",
        "gdf_27700.plot(ax=ax[1], edgecolor='k')\n",
        "ax[1].set_title('British National Grid (EPSG:27700)')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EP4aLd0SyYAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_sales['Sales_Count_Diff'] = weekly_sales['Sales_Count'].diff().dropna()"
      ],
      "metadata": {
        "id": "pO7nOSPXil54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "p = d = q = range(0, 3)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 52) for x in pdq]\n",
        "\n",
        "best_aic = float(\"inf\")\n",
        "best_pdq = None\n",
        "best_seasonal_pdq = None\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        try:\n",
        "            mod = SARIMAX(weekly_sales['Sales_Count'], order=param, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "            if results.aic < best_aic:\n",
        "                best_aic = results.aic\n",
        "                best_pdq = param\n",
        "                best_seasonal_pdq = param_seasonal\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f'Best ARIMA Parameters: {best_pdq} with seasonal {best_seasonal_pdq}')"
      ],
      "metadata": {
        "id": "ba4ssmyvil2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "muzbrLDAilzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weekly_sales= backup_weekly_sales.copy()\n",
        "\n",
        "weekly_sales = weekly_sales.drop('Sales_MA', axis=1)\n",
        "# Set the index to Week_Start for time series analysis\n",
        "weekly_sales.set_index('Week_Start', inplace=True)\n",
        "\n",
        "\n",
        "# Fit the ARIMA model (order parameters can be tuned)\n",
        "arima_model = ARIMA(weekly_sales['Sales_Count'], order=(5, 1, 2))\n",
        "arima_result = arima_model.fit()\n",
        "\n",
        "# Forecast future sales\n",
        "\n",
        "# Forecast future sales\n",
        "forecast = arima_result.get_forecast(steps=52)  # Forecast for one year\n",
        "forecast_index = pd.date_range(start=weekly_sales.index[-1], periods=52, freq='W-MON')\n",
        "forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)\n",
        "\n",
        "# Plot the original data and forecast\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(weekly_sales['Sales_Count'], label='Observed')\n",
        "plt.plot(forecast_series, color='red', label='Forecast')\n",
        "plt.title('Sales Forecast using ARIMA')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hvV7Ce0BopP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Load the data (assuming weekly_sales DataFrame with 'Week_Start' as datetime and 'Sales_Count')\n",
        "weekly_sales = backup_weekly_sales.copy()\n",
        "weekly_sales = weekly_sales.drop('Sales_MA', axis=1)\n",
        "weekly_sales = weekly_sales.join(avg_price_per_week['Average_Price'], how='left')\n",
        "\n",
        "weekly_sales.set_index('Week_Start', inplace=True)\n",
        "weekly_sales"
      ],
      "metadata": {
        "id": "0oTfr39COucQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "endog = weekly_sales['Sales_Count']\n",
        "exog = weekly_sales[['Average_Price']]\n",
        "\n",
        "# Fit the SARIMA model on the training set\n",
        "sarima_model = SARIMAX(endog, exog=exog, order=(2, 1, 2), seasonal_order=(1, 1, 1, 52))\n",
        "sarima_result = sarima_model.fit(disp=False)\n",
        "\n",
        "\n",
        "# Forecast future sales with exogenous variables\n",
        "forecast_steps = 52  # Forecast for one year\n",
        "future_exog = exog.iloc[-1:].append(pd.DataFrame({'Average_Price': exog['Average_Price'].iloc[-1]}, index=pd.date_range(start=exog.index[-1] + pd.Timedelta(weeks=1), periods=forecast_steps, freq='W-MON')))\n",
        "\n",
        "forecast = sarima_result.get_forecast(steps=forecast_steps, exog=future_exog)\n",
        "forecast_index = pd.date_range(start=weekly_sales.index[-1] + pd.Timedelta(weeks=1), periods=forecast_steps, freq='W-SUN')\n",
        "forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)\n",
        "forecast_ci = forecast.conf_int()\n",
        "\n",
        "# Plot the original data and forecast\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(weekly_sales['Sales_Count'], label='Observed')\n",
        "plt.plot(forecast_series, color='red', label='Forecast')\n",
        "plt.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='pink', alpha=0.3)\n",
        "plt.title('Sales Forecast using SARIMA with Exogenous Variable (Average Price)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print the forecasted values for verification\n",
        "print(forecast_series)\n"
      ],
      "metadata": {
        "id": "9hu34GltopNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnose the model\n",
        "print(sarima_result.summary())\n",
        "residuals = sarima_result.resid\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(residuals)\n",
        "plt.title('Residuals of ARIMA Model')\n",
        "plt.show()\n",
        "\n",
        "# Plot ACF of residuals to check if they resemble white noise\n",
        "plot_acf(residuals, lags=50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WdFYWDm8US5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Fit the SARIMA model (with seasonal components if necessary)\n",
        "sarima_model = SARIMAX(weekly_sales['Sales_Count'], order=(2, 1, 2), seasonal_order=(1, 1, 1, 52))\n",
        "sarima_result = sarima_model.fit(disp=False)\n",
        "\n",
        "# Forecast future sales\n",
        "forecast_steps = 52  # Forecast for one year\n",
        "forecast = sarima_result.get_forecast(steps=forecast_steps)\n",
        "forecast_index = pd.date_range(start=weekly_sales.index[-1] + pd.Timedelta(weeks=1), periods=forecast_steps, freq='W-MON')\n",
        "forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)\n",
        "forecast_ci = forecast.conf_int()\n",
        "\n",
        "# Plot the original data and forecast\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(weekly_sales['Sales_Count'], label='Observed')\n",
        "plt.plot(forecast_series, color='red', label='Forecast')\n",
        "plt.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='pink', alpha=0.3)\n",
        "plt.title('Sales Forecast using SARIMA')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Diagnose the model\n",
        "print(sarima_result.summary())\n",
        "residuals = sarima_result.resid\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(residuals)\n",
        "plt.title('Residuals of ARIMA Model')\n",
        "plt.show()\n",
        "\n",
        "# Plot ACF of residuals to check if they resemble white noise\n",
        "plot_acf(residuals, lags=50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GD0OfulYUS2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "P_VIOw4DcYCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "train_size = int(len(weekly_sales) * 0.8)\n",
        "train, test = weekly_sales.iloc[:train_size], weekly_sales.iloc[train_size:]\n",
        "\n",
        "\n",
        "# Fit the SARIMA model on the training set\n",
        "sarima_model = SARIMAX(train['Sales_Count'], order=(3, 1, 2), seasonal_order=(1, 1, 1, 52))\n",
        "sarima_result = sarima_model.fit(disp=False)\n",
        "\n",
        "# Forecast the test set\n",
        "forecast_steps = len(test)\n",
        "forecast = sarima_result.get_forecast(steps=forecast_steps)\n",
        "forecast_index = test.index\n",
        "forecast_series = pd.Series(forecast.predicted_mean, index=forecast_index)\n",
        "forecast_ci = forecast.conf_int()\n",
        "\n",
        "# Plot the train, test, and forecast\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(train['Sales_Count'], label='Train')\n",
        "plt.plot(test['Sales_Count'], label='Test', color='orange')\n",
        "plt.plot(forecast_series, color='red', label='Forecast')\n",
        "plt.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='pink', alpha=0.3)\n",
        "plt.title('Train/Test Split with SARIMA Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print forecast accuracy metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "mse = mean_squared_error(test['Sales_Count'], forecast_series)\n",
        "mae = mean_absolute_error(test['Sales_Count'], forecast_series)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'Mean Absolute Error: {mae}')\n"
      ],
      "metadata": {
        "id": "TYaD5RlJUS0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "weekly_sales= backup_weekly_sales.copy()\n",
        "weekly_sales = weekly_sales.drop('Sales_MA', axis=1)\n",
        "weekly_sales.set_index('Week_Start', inplace=True)\n",
        "\n",
        "# Define the p, d, q parameters to take any value between 0 and 2\n",
        "p = d = q = range(0, 3)\n",
        "# Generate all different combinations of p, d and q triplets\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "# Generate all different combinations of seasonal p, d and q triplets with seasonal period 52\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 52) for x in pdq]\n",
        "\n",
        "best_aic = float(\"inf\")\n",
        "best_pdq = None\n",
        "best_seasonal_pdq = None\n",
        "\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        try:\n",
        "            mod = SARIMAX(weekly_sales['Sales_Count'], order=param, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "            if results.aic < best_aic:\n",
        "                best_aic = results.aic\n",
        "                best_pdq = param\n",
        "                best_seasonal_pdq = param_seasonal\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f'Best ARIMA Parameters: {best_pdq} with seasonal {best_seasonal_pdq}')\n"
      ],
      "metadata": {
        "id": "u28WrkcBUSx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0gPxpGWUSvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_in_range(laua):\n",
        "    try:\n",
        "        # Extract the numeric part and convert to integer\n",
        "        laua_num = int(laua[1:])\n",
        "        # Check if it is within the range\n",
        "        return 9000001 <= laua_num <= 9000033\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# Apply the function to the 'laua' column\n",
        "filtered_nspl_df = nspl_df[nspl_df['laua'].apply(is_in_range)]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "print(\"Records where 'laua' values are between E09000001 and E09000033:\")\n",
        "print(filtered_nspl_df)"
      ],
      "metadata": {
        "id": "qDqBWpXyKq75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Ru1JgPg51jsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bacckup = df.copy()"
      ],
      "metadata": {
        "id": "JxzKqG0nE_w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "lreBSMCV4zmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "XUatEEJuoFCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Price']>8.0e+08]"
      ],
      "metadata": {
        "id": "nKr5XY8KoKuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = df[\n",
        "    (df['Postcode'] == 'TN23 7HE')\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "nN13e1Q2olBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered"
      ],
      "metadata": {
        "id": "h_00-Fmyo20p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Category_B = df[df['PPD_Category_Type']=='B']\n",
        " Category_B_sorted = Category_B.sort_values(by='Date_of_Transfer')"
      ],
      "metadata": {
        "id": "LY8oe3XQpb-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Category_B['PPD_Category_Type'].value_counts()"
      ],
      "metadata": {
        "id": "0Kdbs124qL_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff_date = pd.Timestamp('2013-10-01')\n",
        "Category_B_sorted[Category_B_sorted['Date_of_Transfer'] < cutoff_date]"
      ],
      "metadata": {
        "id": "X7zpPtWip7fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\n",
        "    'Property_Type',\n",
        "    'Old/New',\n",
        "    'Duration',\n",
        "    'PPD_Category_Type',\n",
        "    'Record_Status_monthly_file_only'\n",
        "]\n",
        "\n",
        "Address_columns = [\n",
        "    'SAON',\n",
        "    'PAON',\n",
        "    'Street',\n",
        "    'Locality',\n",
        "    'Town/City',\n",
        "    'District',\n",
        "    'County',\n",
        "    'Postcode'\n",
        "]\n",
        "\n",
        "Transaction_columns = [\n",
        "    'Transaction_unique_identifier',\n",
        "    'Price',\n",
        "    'Date_of_Transfer'\n",
        "]"
      ],
      "metadata": {
        "id": "sqmYvhzoxzJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate = df[df.duplicated(subset=['SAON',\n",
        "    'Property_Type',\n",
        "    'Old/New',\n",
        "    'Duration',\n",
        "    'PPD_Category_Type',\n",
        "    'PAON',\n",
        "    'Street',\n",
        "    'Locality',\n",
        "    'Town/City',\n",
        "    'District',\n",
        "    'County',\n",
        "    'Postcode',\n",
        "    'Date_of_Transfer'], keep=False)]"
      ],
      "metadata": {
        "id": "muUDzgLz5Mpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate"
      ],
      "metadata": {
        "id": "l_JqU_1M6hxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific values to check\n",
        "record_to_check = {\n",
        "    'SAON': 'APARTMENT 5',\n",
        "    'PAON': '100',\n",
        "    'Street': 'SILLAVAN WAY',\n",
        "    'Locality': pd.NA,\n",
        "    'Town/City': 'BARNET',\n",
        "    'District': 'BARNET',\n",
        "    'County': 'GREATER MANCHESTER',\n",
        "    'Postcode': 'CF63 1BB'\n",
        "}"
      ],
      "metadata": {
        "id": "0IJotgoXGsM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame for the specific record\n",
        "filtered_df = df[\n",
        "    (df['Town/City'] == record_to_check['Town/City'])\n",
        "]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "print(\"Records matching the specified values:\")\n",
        "filtered_df"
      ],
      "metadata": {
        "id": "Kceyfop8GG2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = address_df.duplicated()"
      ],
      "metadata": {
        "id": "16d2qMjx6nfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_entries = df[df.duplicated(subset=['Date_of_Transfer', 'Postcode','SAON', 'PAON','Street','Locality','Town/City','District','County', 'Property_Type', 'Old/New', 'Price'], keep=False)]\n",
        "print(\"Potential duplicate records:\")\n",
        "duplicate_entries"
      ],
      "metadata": {
        "id": "3KfzzpUp90OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inconsistencies = df[(df['Old/New'] == 'N') & (df['Date_of_Transfer'] < '2000-01-01')]\n",
        "print(\"Potential inconsistent records:\")\n",
        "inconsistencies"
      ],
      "metadata": {
        "id": "bJMdCZ4mEBcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[dup]"
      ],
      "metadata": {
        "id": "AtQbl1l-96Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicated_df = df[duplicate_rows]"
      ],
      "metadata": {
        "id": "AGRSzlmH6rGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicated_df['Duration'].value_counts()"
      ],
      "metadata": {
        "id": "2CUI3Krc9TFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to standardize string comparison\n",
        "def standardize_string(s):\n",
        "    if pd.isna(s):\n",
        "        return s\n",
        "    return str(s).strip().upper()\n",
        "\n",
        "# Apply the function to the DataFrame and the record to check\n",
        "for key in record_to_check.keys():\n",
        "    df[key] = df[key].apply(standardize_string)\n",
        "    record_to_check[key] = standardize_string(record_to_check[key])"
      ],
      "metadata": {
        "id": "_J1_LAqr0miM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lease_houses = df[df['Duration'] == 'L']\n",
        "\n",
        "# Find duplicate addresses and calculate the date difference\n",
        "lease_houses_sorted = lease_houses.sort_values(by=['PAON', 'SAON', 'Postcode', 'Date_of_Transfer'])\n",
        "lease_houses_sorted['Date_Diff_Days'] = lease_houses_sorted.groupby(['PAON', 'SAON', 'Postcode'])['Date_of_Transfer'].diff().dt.days\n",
        "\n",
        "# Display the DataFrame with the new 'Date_Diff_Days' column\n",
        "print(\"Lease houses with date difference in days:\")\n",
        "lease_houses_sorted"
      ],
      "metadata": {
        "id": "pFfRsM9aHol_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lease_houses_sorted[lease_houses_sorted['Date_Diff_Days']<2555]"
      ],
      "metadata": {
        "id": "oUt6FLGBIZzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_duplicates = duplicates.groupby(Address_columns).size().reset_index(name='Counts')"
      ],
      "metadata": {
        "id": "o8REIEjg1C9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_duplicates1 = grouped_duplicates[grouped_duplicates['Counts'] > 1]\n",
        "grouped_duplicates1"
      ],
      "metadata": {
        "id": "S8vm-LUR1bwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Loop through the categorical columns and plot the value counts\n",
        "for i, column in enumerate(categorical_columns, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.countplot(y=df[column], order=df[column].value_counts().index)\n",
        "    plt.title(f'Value Counts of {column}')\n",
        "    plt.xlabel('Counts')\n",
        "    plt.ylabel(column)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SbRi2S8Cybmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Duration'].value_counts()"
      ],
      "metadata": {
        "id": "Jsx4SCMvdQtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Old/New'].value_counts()"
      ],
      "metadata": {
        "id": "GqBxZmCy0J5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))\n",
        "df['unknown'].value_counts()"
      ],
      "metadata": {
        "id": "8zI4yZd0waox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "IQG9E_eZwpHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "-ilzxSH62Rj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "6cxy7wgl3bt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy()"
      ],
      "metadata": {
        "id": "gTNd8hok9ceE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Price'].describe()"
      ],
      "metadata": {
        "id": "QPIzeXNh9x4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[df1['Price']<100]"
      ],
      "metadata": {
        "id": "4QgwZgKp-qYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicated_transactions = df1[df1.duplicated(subset=['Transaction_unique_identifier'], keep=False)]\n",
        "\n",
        "# Check if there are any duplicates and display them\n",
        "if not duplicated_transactions.empty:\n",
        "    print(\"Duplicates found in Transaction unique identifier:\")\n",
        "    print(duplicated_transactions)\n",
        "else:\n",
        "    print(\"No duplicates found in Transaction unique identifier.\")"
      ],
      "metadata": {
        "id": "lHMiF-K--yPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_entries = df1[df1.duplicated(subset=[\"PAON\",\"SAON\",\"Street\",\"Locality\",\"Town/City\",\"District\",\"County\",\"Postcode\"], keep=False)]\n",
        "print(\"Potential duplicate records:\")\n",
        "print(duplicate_entries)"
      ],
      "metadata": {
        "id": "jI2yX0oy_kF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf1 = gpd.read_file('/content/drive/MyDrive/Chimnie/OSGB_Grids-master/GeoJSON/OSGB_Grid_10km.geojson')"
      ],
      "metadata": {
        "id": "qZPSE1-DAq0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf1.info()"
      ],
      "metadata": {
        "id": "gc2mGN77mznQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gdf1.crs)"
      ],
      "metadata": {
        "id": "T8IwG1Lom11d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_region(row):\n",
        "    if row['ENGLAND'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'England'\n",
        "    elif row['SCOTLAND'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'Scotland'\n",
        "    elif row['WALES'] == 't':  # Assuming '1' indicates presence\n",
        "        return 'Wales'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "gdf1['REGION'] = gdf1.apply(determine_region, axis=1)\n",
        "\n",
        "# Plotting the GeoDataFrame with different colors for each region\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "gdf1.plot(column='REGION', ax=ax, legend=True, cmap='Set3', edgecolor='k')\n",
        "\n",
        "# Setting title and labels\n",
        "ax.set_title('10km Map of England, Scotland, and Wales')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N3M3SWeunkO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XjnRwiwntvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}